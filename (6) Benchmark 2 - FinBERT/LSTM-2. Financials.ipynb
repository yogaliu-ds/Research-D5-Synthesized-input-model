{"cells":[{"cell_type":"markdown","metadata":{"id":"iiiO7bcHN_Ao"},"source":["# 0 Setting"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3739,"status":"ok","timestamp":1703645915245,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"aWHVuAauNB5o","outputId":"ae1ab601-6ad4-400b-9fa9-bbb90b21fd2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["11\n"]}],"source":["# Parameter Setting\n","import torch\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","folder_name = 'Research-(D5) Synthesized input model'\n","pretrained_model_name = 'model_microsoft.ckpt'\n","\n","config = {\n","    'learning_rate': 4e-3,\n","    'batch_size': 32,\n","    'seq_length': 5,\n","\n","    'shuffle': False,\n","    'criterion': torch.nn.CrossEntropyLoss(),\n","    'seed': 42,\n","\n","    'n_epochs': 3000,\n","    'early_stop': 500,\n","    'device': device,\n","}\n","\n","\n","sector_id_list = [ # Not confirmed\n","    \"XLK\",  # Information Technology\n","    \"XLV\",  # Health Care\n","    \"XLF\",  # Financials\n","    \"XLI\",  # Industrials\n","    \"XLY\",  # Consumer Discretionary\n","    \"XLE\",  # Energy\n","    \"XLB\",  # Materials\n","    \"XLC\",  # Communication Services\n","    \"XLU\",  # Utilities\n","    \"XLRE\",  # Real Estate\n","    \"XLP\"  # Consumer Staples\n","]\n","\n","company_list = [\n","    \"Information Technology\",\n","    \"Health Care\",\n","    \"Financials\",\n","    \"Industrials\",\n","    \"Consumer Discretionary\",\n","    \"Energy\",\n","    \"Materials\",\n","    \"Communication Services\",\n","    \"Utilities\",\n","    \"Real Estate\",\n","    \"Consumer Staples\"\n","]\n","\n","process_id = 2  #26\n","\n","company_name = company_list[process_id]\n","\n","config_2 = {'input_path': '/content/drive/MyDrive/Colab Notebooks/'+folder_name+'/data/2_'+company_name+'_for_model.csv',\n","            'save_path': '/content/drive/MyDrive/Colab Notebooks/'+folder_name+'/model_saved/model_'+company_name+'.ckpt',\n","            # 'pretrained_model_path': '/content/drive/MyDrive/Colab Notebooks/'+folder_name+'/premodel/' + pretrained_model_name,\n","            # 'continue_model_path': '/content/drive/MyDrive/Colab Notebooks/'+folder_name+'/model_saved/model_1.ckpt'\n","            }\n","\n","feature = [\n","    # X_1\n","    'input_ids',\n","    'attention_mask',\n","    'section_dummy',\n","    'publication_dummy',\n","\n","    # X_2\n","    # 1. tech indicator\n","    # 'Open',\n","    # 'High',\n","    # 'Low',\n","    # 'Close',\n","    # 'Volume',\n","    # 'Dividends',\n","    # 'Stock Splits',\n","    'today_return',\n","    # 'today_return_cate',\n","    # 'Sma',\n","    # 'Rsi',\n","    # 'Kd',\n","    # 'Ema_12',\n","    # 'Ema_26',\n","    # 'Macd',\n","    # 'sentiment',\n","\n","    # 'logit0_bert',\n","    # 'logit1_bert',\n","    # 'logit0_finbert',\n","    # 'logit1_finbert',\n","    # 'logit2_finbert',\n","\n","    # 'logit0_bert_average',\n","    # 'logit1_bert_average',\n","    # 'logit0_finbert_average',\n","    # 'logit1_finbert_average',\n","    # 'logit2_finbert_average',\n","\n","    # 2. market index\n","    '^DJI',\n","    '^GSPC',\n","    '^NDX',\n","    '^IXIC',\n","    '^SOX',\n","    '^NYA',\n","\n","    # y\n","    # '1_day_return',\n","    # '2_day_return',\n","    # '3_day_return',\n","    # '4_day_return',\n","    # '5_day_return',\n","    # '1_day_return_cate',\n","    # '2_day_return_cate',\n","    # '3_day_return_cate',\n","    # '4_day_return_cate',\n","    # '5_day_return_cate',\n","    # '^DJI', '^DJI_1_day_return', '^GSPC', '^GSPC_1_day_return',\n","    #    '^NDX', '^NDX_1_day_return', '^IXIC', '^IXIC_1_day_return', '^SOX',\n","    #    '^SOX_1_day_return',\n","    # 'excess_return_^DJI',\n","    # 'excess_return_^DJI_cate',\n","    # 'excess_return_^GSPC',\n","    'excess_return_^GSPC_cate',\n","    # 'excess_return_^NDX',\n","    # 'excess_return_^NDX_cate',\n","    # 'excess_return_^IXIC',\n","    # 'excess_return_^IXIC_cate',\n","    # 'excess_return_^SOX',\n","    # 'excess_return_^SOX_cate',\n","\n","    # Do not mark the datetime, it's for operation\n","    'datetime',\n","    ]\n","\n","# All the news dataset\n","# time_start = '2016-01-01T00:00:00'\n","# time_end = '2020-04-02T00:00:00'\n","\n","time_start = '2016-01-01T00:00:00'\n","time_end = '2019-12-31T00:00:00'\n","\n","print(len(feature)-2)"]},{"cell_type":"markdown","metadata":{"id":"Paj5_XoohZwu"},"source":["## (1) Import"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28593,"status":"ok","timestamp":1703645943834,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"Ca00OwVROzyQ","outputId":"857a8bec-ea0a-40b0-d2f0-a619e02b969a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"]}],"source":["# Google\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# pip installation\n","!pip install transformers\n","\n","# Basic\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import math\n","\n","# Sklearn\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","\n","# PyTorch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from transformers import BertTokenizer, BertModel, BertConfig\n","\n","# others\n","from datetime import datetime, timedelta\n","from tqdm import tqdm\n","from torchsummary import summary\n","import ast"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hnPf7jxjPPDL","executionInfo":{"status":"ok","timestamp":1703645943834,"user_tz":-540,"elapsed":12,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["def same_seed(seed):\n","    '''Fixes random number generator seeds for reproducibility.'''\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","# Set seed for reproducibility\n","same_seed(config['seed'])\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"elapsed":1181,"status":"error","timestamp":1703645945007,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"zH3oxSTkO85T","outputId":"06f7568f-158f-44d6-850b-9cf243023484"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-daeeeff425ca>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/Research-(D5) Synthesized input model/data/2_Financials_for_model.csv'"]}],"source":["df = pd.read_csv(config_2['input_path'])\n","df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"aborted","timestamp":1703645945008,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"184Zrsi8LD02"},"outputs":[],"source":["df = df.sort_values(by='datetime', ascending=True)\n","df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"aborted","timestamp":1703645945008,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"11JAV-xePHKF"},"outputs":[],"source":["# Only contain selected features\n","df = df[feature]\n","df.columns"]},{"cell_type":"markdown","metadata":{"id":"Vkha6OadTpMA"},"source":["## (2) check nan"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":19,"status":"aborted","timestamp":1703645945008,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"QKx4PK-UEgb7"},"outputs":[],"source":["df[df.isna().any(axis=1)]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"aborted","timestamp":1703645945009,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"nhfwhVrqTrY5"},"outputs":[],"source":["df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":19,"status":"aborted","timestamp":1703645945009,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"w4i2TznkTs5o"},"outputs":[],"source":["df = df.dropna()\n","df = df.reset_index(drop=True)\n","df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"qtUEqIEXkOHE"},"source":["## (2) Time Period Selection"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1703645945009,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"09E8ivNfPwtB"},"outputs":[],"source":["# We use index to filter for time periods\n","df = df[(df['datetime']> time_start) & (df['datetime'] < time_end)]\n","\n","# Drop datetime after using it\n","df.drop(columns=['datetime'], inplace=True)\n","df.shape"]},{"cell_type":"markdown","metadata":{"id":"7rdZgCpbe8DQ"},"source":["## (3) Transform str back to list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WDujbxYKe6qj","executionInfo":{"status":"aborted","timestamp":1703645945009,"user_tz":-540,"elapsed":18,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["# 将字符串转换回列表的函数\n","def string_to_list(s):\n","    return ast.literal_eval(s)\n","\n","# 将列中的字符串转换回列表\n","df['input_ids'] = df['input_ids'].apply(string_to_list)\n","df['attention_mask'] = df['attention_mask'].apply(string_to_list)\n","df['section_dummy'] = df['section_dummy'].apply(string_to_list)\n","df['publication_dummy'] = df['publication_dummy'].apply(string_to_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1703645945009,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"zGHAxc3oGUjH"},"outputs":[],"source":["df['input_ids'][0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1703645945010,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"IMoH702QNmLZ"},"outputs":[],"source":["df['attention_mask'][0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1703645945010,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"CeeEKFq5NpC3"},"outputs":[],"source":["df['section_dummy'][0][0]"]},{"cell_type":"markdown","metadata":{"id":"tzhAx562jOlb"},"source":["## (3) List: Same amount of elements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBwY00GFymVO","executionInfo":{"status":"aborted","timestamp":1703645945010,"user_tz":-540,"elapsed":18,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["input_ids_list_length = len(df['input_ids'][0][0])\n","attention_mask_list_length = len(df['attention_mask'][0][0])\n","section_dummy_list_length = len(df['section_dummy'][0][0])\n","publication_dummy_list_length = len(df['publication_dummy'][0][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1703645945010,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"-mGgQZ6ojU5c"},"outputs":[],"source":["# 找到最大的內部列表長度\n","max_inner_length = max(df['input_ids'].apply(len))\n","\n","# 定義一個函數來填充內部列表，使其長度達到最大值\n","def pad_inner_list(lst, zero_list):\n","    while len(lst) < max_inner_length:\n","        lst.append(zero_list)  # 這裡可以填充任何你想要的值，例如 None\n","\n","# 將 \"input_ids\" 列中的每個內部列表填充到最大長度\n","df['input_ids'].apply(pad_inner_list, zero_list=[0] * input_ids_list_length)\n","df['attention_mask'].apply(pad_inner_list, zero_list=[0] * attention_mask_list_length)\n","df['section_dummy'].apply(pad_inner_list, zero_list=[0] * section_dummy_list_length)\n","df['publication_dummy'].apply(pad_inner_list, zero_list=[0] * publication_dummy_list_length)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1703645945010,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"rGW3R5I-zoAV"},"outputs":[],"source":["df['section_dummy'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1703645945010,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"nemqwoqfj5gM"},"outputs":[],"source":["# 使用 apply 方法計算每個列表中元素的數量\n","# Note: This number is not about tokens. It's about the number of news in that day.\n","temp = df['input_ids'].apply(len)\n","\n","# 打印 DataFrame\n","temp"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1703645945011,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"gjCUj07CIYtP"},"outputs":[],"source":["# 計算\"input_ids\"列中所有list的平均長度\n","average_length = df['input_ids'].apply(len).mean()\n","\n","# 計算\"input_ids\"列中最長的list的長度\n","max_length = df['input_ids'].apply(len).max()\n","\n","# 計算\"input_ids\"列中最短的list的長度\n","min_length = df['input_ids'].apply(len).min()\n","\n","# 打印結果\n","print(f\"平均長度: {average_length}\")\n","print(f\"最長長度: {max_length}\")\n","print(f\"最短長度: {min_length}\")\n"]},{"cell_type":"markdown","metadata":{"id":"2GqgLTLStucf"},"source":["## int to float (section, publication)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1703645945011,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"Le2gfI67Mel-"},"outputs":[],"source":["type(df.input_ids[0][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhEV-SK6tak0","executionInfo":{"status":"aborted","timestamp":1703645945011,"user_tz":-540,"elapsed":18,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["def recursive_convert_to_float(item):\n","    if isinstance(item, list):\n","        return [recursive_convert_to_float(x) if x is not None else None for x in item]\n","    else:\n","        return float(item) if item is not None else None\n","\n","# 使用 apply 方法將函數應用於每個元素\n","df['section_dummy'] = df['section_dummy'].apply(recursive_convert_to_float)\n","df['publication_dummy'] = df['publication_dummy'].apply(recursive_convert_to_float)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1703645945011,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"Nowtpnj4tzgo"},"outputs":[],"source":["df['section_dummy'][0]"]},{"cell_type":"markdown","metadata":{"id":"4Q7dnpuMeJGj"},"source":["## (4) Train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lNUztpwP1pY","executionInfo":{"status":"aborted","timestamp":1703645945011,"user_tz":-540,"elapsed":17,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["# 1. Set up X, y\n","to_remove_list = ['datetime', 'excess_return_^GSPC_cate']\n","\n","# Filter out values in to_remove_list\n","filtered_list = [x for x in feature if x not in to_remove_list]\n","\n","X = df[filtered_list]\n","y = df['excess_return_^GSPC_cate']"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1703645945011,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"TuCa3Cs0P4lN"},"outputs":[],"source":["# Check X, y shape\n","print('X:', X.shape)\n","print('y:', y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1703645945011,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"NBW9qnq5QASt"},"outputs":[],"source":["# 2. train_test_split\n","# val dataset for final examination\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=config['seed'], shuffle=config['shuffle'])\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=config['seed'], shuffle=config['shuffle'])\n","\n","# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=config['seed'], shuffle=config['shuffle'])\n","# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.66, random_state=config['seed'], shuffle=config['shuffle'])\n","X_train\n"]},{"cell_type":"markdown","metadata":{"id":"RccwSqS3SOmR"},"source":["## (5) Scaler"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1703645945011,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"-wg4fy92Q81E"},"outputs":[],"source":["scale_feature = [\n","    # X_2\n","    # 1. tech indicator\n","    # 'Open',\n","    # 'High',\n","    # 'Low',\n","    # 'Close',\n","    # 'Volume',\n","    # 'Dividends',\n","    # 'Stock Splits',\n","    'today_return',\n","    # 'Today_trend_cate',\n","    # 'Sma',\n","    # 'Rsi',\n","    # 'Kd',\n","    # 'Ema_12',\n","    # 'Ema_26',\n","    # 'Macd',\n","    # 'sentiment',\n","\n","    # 'logit0_bert',\n","    # 'logit1_bert',\n","    # 'logit0_finbert',\n","    # 'logit1_finbert',\n","    # 'logit2_finbert',\n","\n","    # 'logit0_bert_average',\n","    # 'logit1_bert_average',\n","    # 'logit0_finbert_average',\n","    # 'logit1_finbert_average',\n","    # 'logit2_finbert_average',\n","\n","    # 2. market index\n","    '^DJI',\n","    '^GSPC',\n","    '^NDX',\n","    '^IXIC',\n","    '^SOX',\n","    '^NYA',\n","\n","    # 'datetime'\n","    ]\n","\n","def CustomScaler(X_train, X_val, X_test):\n","  scaler = MinMaxScaler()\n","  for i in scale_feature:\n","\n","    # 對特定欄位進行標準化\n","    X_train_scaled = scaler.fit_transform(X_train[[i]])\n","    X_val_scaled = scaler.transform(X_val[[i]])\n","    X_test_scaled = scaler.transform(X_test[[i]])\n","\n","    # 將標準化後的值重新賦值給 DataFrame\n","    X_train[i] = X_train_scaled\n","    X_val[i] = X_val_scaled\n","    X_test[i] = X_test_scaled\n","\n","  return X_train, X_val, X_test\n","\n","X_train, X_val, X_test = CustomScaler(X_train, X_val, X_test)\n","\n","X_train"]},{"cell_type":"markdown","metadata":{"id":"pGscrhKkRNxo"},"source":["## (6) Check number"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1703645945011,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"E6pIRzzhRMoj"},"outputs":[],"source":["def calculate_class_stats(y):\n","    class_counts = y.value_counts()\n","    total_samples = len(y)\n","    class_ratios = class_counts / total_samples\n","    return class_counts, class_ratios\n","\n","# 計算類別數量和比例\n","train_class_counts, train_class_ratios = calculate_class_stats(y_train)\n","val_class_counts, val_class_ratios = calculate_class_stats(y_val)\n","test_class_counts, test_class_ratios = calculate_class_stats(y_test)\n","\n","# 創建包含數量和比例的 DataFrame\n","class_stats = pd.DataFrame({\n","    'Train Count': train_class_counts,\n","    'Train Ratio': train_class_ratios,\n","    'Validation Count': val_class_counts,\n","    'Validation Ratio': val_class_ratios,\n","    'Test Count': test_class_counts,\n","    'Test Ratio': test_class_ratios\n","})\n","\n","# 打印 DataFrame\n","print(class_stats)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1703645945011,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"SsUxmaKYREFr"},"outputs":[],"source":["# Time period\n","print('Time Period')\n","print('From:', time_start)\n","print('To:', time_end, '\\n')\n","\n","# Sample size\n","print('Sample size:', X.shape[0])\n","print('Feature:', X.columns, '\\n')\n","print('Target:', y.name, '\\n')\n","print('Train: Val: Test =', X_train.shape[0], X_val.shape[0], X_test.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"Sd441E-MR_SU"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"dwBFKDyGTyFq"},"source":["## (1) Dataset & Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KAcfnWaRHN6","executionInfo":{"status":"aborted","timestamp":1703645945012,"user_tz":-540,"elapsed":17,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["# Dataset\n","X_1 =['input_ids', 'attention_mask', 'section_dummy', 'publication_dummy']\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, X, y, config):\n","        # X_1\n","        self.input_ids = X['input_ids']\n","        self.attention_mask = X['attention_mask']\n","\n","        # X_2\n","        self.X_2 = torch.tensor(X.drop(columns=X_1).values, dtype=torch.float)\n","\n","        # y\n","        self.y = torch.tensor(y.values, dtype=torch.long)\n","\n","        # other setting\n","        self.len = X.shape[0]\n","        self.seq_length = config['seq_length']\n","\n","    def __getitem__(self,idx):\n","        # X_1\n","        input_ids_list = self.input_ids[idx : idx + self.seq_length].tolist() # All to list\n","        input_ids = torch.tensor(input_ids_list) # Then to tensor\n","        attention_mask_list = self.attention_mask[idx : idx + self.seq_length].tolist()\n","        attention_mask = torch.tensor(attention_mask_list)\n","\n","        # X_2\n","        X_2 = self.X_2[idx : idx + self.seq_length]\n","\n","        # 3. y\n","        y = self.y[idx + self.seq_length - 1]\n","\n","        return input_ids, attention_mask, X_2, y\n","\n","    def __len__(self):\n","        return self.len - self.seq_length"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1703645945012,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"aL5Dlz21VFzy"},"outputs":[],"source":["# DataLoader\n","train_dataset = CustomDataset(X_train, y_train, config)\n","val_dataset = CustomDataset(X_val, y_val, config)\n","test_dataset = CustomDataset(X_test, y_test, config)\n","\n","train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=config['shuffle'], drop_last=True, pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=config['shuffle'], drop_last=True, pin_memory=True)\n","test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=config['shuffle'], drop_last=True, pin_memory=True)\n","\n","# Check loader output\n","for batch in train_loader:\n","    input_ids, attention_mask, X_2, y = batch\n","\n","    # 打印批次数据的形状，以确保它们符合预期\n","    print(\"Input IDs shape:\", input_ids.shape)\n","    print(\"Attention Mask shape:\", attention_mask.shape)\n","    print(\"X_2 shape:\", X_2.shape)\n","    print(\"Labels shape:\", y.shape)\n","\n","    # print(\"Input IDs:\", input_ids)\n","    # print(\"Attention Mask:\", attention_mask)\n","    # print(\"Section:\", section)\n","    # print(\"Publication:\", publication)\n","    # print(\"X_2:\", X_2)\n","    # print(\"Labels:\", y)\n","\n","    break\n"]},{"cell_type":"markdown","metadata":{"id":"_bL0C5M_TtK7"},"source":["## (2) Model Architecture"]},{"cell_type":"markdown","metadata":{"id":"JpX2Eo_HtqMv"},"source":["### 0 Param setting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJewmLfayUA1","executionInfo":{"status":"aborted","timestamp":1703645945012,"user_tz":-540,"elapsed":17,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["new_config = {\n","    'h_text_size': 5,\n","    'h_c_size': 1,\n","}\n","\n","config.update(new_config)"]},{"cell_type":"markdown","metadata":{"id":"n7nD73XMqu5A"},"source":["### 1 New design"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zgdm3rwtqzmG","executionInfo":{"status":"aborted","timestamp":1703645945012,"user_tz":-540,"elapsed":17,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["# New method: C as a scaler + activation function + layer normalization\n","class MyModel(nn.Module):\n","    def __init__(self, base_model, config, element_size, X_2_length, batch_size):\n","        super(MyModel, self).__init__()\n","        self.seq_length = config['seq_length']\n","        self.batch_size = batch_size\n","        self.element_size = element_size\n","        self.abandon_tensor = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                              0, 0, 0, 0, 0, 0, 0, 0], device=device)\n","        self.config = config\n","\n","        # 1. News\n","        # text\n","        self.base_model = base_model\n","        self.fc1 = nn.Linear(768, config['h_text_size'])\n","\n","        # 2. LSTM\n","        self.lstm_1 = nn.LSTM(config['h_text_size'] + X_2_length, 32, dropout=0.1, num_layers=2, batch_first=True, bidirectional=False)\n","        self.sequential = nn.Sequential(\n","            nn.Linear(32, 2)\n","        )\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","        self.batch_norm = nn.BatchNorm1d(config['h_text_size'])\n","\n","\n","\n","    def forward(self, input_ids, attention_mask, X_2):\n","        # 1. BERT\n","        flattened_input_ids = input_ids.view(-1, 32)\n","        flattened_attention_mask = attention_mask.view(-1, 32)\n","\n","        e_list = []\n","        for i in range(0, flattened_input_ids.size(0), self.element_size):\n","          sub_input_ids = flattened_input_ids[i:i+self.element_size]\n","          sub_attention_mask = flattened_attention_mask[i:i+self.element_size]\n","\n","          non_zero_mask = (sub_input_ids != 0).any(dim=1)\n","          non_zero_input_ids = sub_input_ids[non_zero_mask]\n","          non_zero_attention_mask = sub_attention_mask[non_zero_mask]\n","\n","          # input_ids, attention_mask\n","          out = self.base_model(input_ids=non_zero_input_ids, attention_mask=non_zero_attention_mask)\n","          out = out.pooler_output\n","          h_text = self.fc1(out)\n","\n","          batch_norm_h = self.batch_norm(h_text)\n","          element_mean = torch.mean(batch_norm_h, dim=0)\n","          e_list.append(element_mean)\n","\n","        temp_tensor = torch.stack(e_list)\n","        b_tensor = temp_tensor.view(self.batch_size, self.seq_length, self.config['h_text_size'])\n","\n","        # After BERT\n","\n","        h_tech = X_2\n","\n","        h = torch.cat([b_tensor, h_tech], dim=2)\n","\n","        # 3. LSTM\n","        out, _ = self.lstm_1(h)\n","        out = out[:, -1, :]  # Get the last one of LSTM output for prediction of next-term\n","\n","        final_out = self.sequential(out)\n","\n","        return final_out\n"]},{"cell_type":"markdown","metadata":{"id":"nzn9Gs2qvN5I"},"source":["## (4) Load Model"]},{"cell_type":"markdown","metadata":{"id":"U5KvkYq4PE59"},"source":["### 1. Load pretrain model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1703645945012,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"W7OjGqgVJ38r"},"outputs":[],"source":["# 載入預訓練模型\n","from transformers import AutoModelForSequenceClassification\n","# base_model = PreModel(base_model)\n","# base_model.load_state_dict(torch.load(config_2['pretrained_model_path']))\n","# bert_config = BertConfig(hidden_dropout_prob=0.2)\n","model_name = 'ProsusAI/finbert'\n","# model_name = 'bert-base-uncased'\n","base_model = BertModel.from_pretrained(model_name)\n","# base_model = BertModel.from_pretrained(model_name, config=bert_config)\n","# base_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","\n","# Parameter\n","element_size = len(df['input_ids'][0])  # 114\n","X_2_length = len(feature) - 6\n"]},{"cell_type":"markdown","metadata":{"id":"Rp_BgvqKPIhb"},"source":["### 2. Initiate Model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1703645945012,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"XcR2jjuDrxh6"},"outputs":[],"source":["model = MyModel(base_model, config, element_size, X_2_length, config['batch_size'])\n","\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"S0kKkkcaO5Ma"},"source":["### Extra: Contunue training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3qQOjwKO5bD","executionInfo":{"status":"aborted","timestamp":1703645945012,"user_tz":-540,"elapsed":17,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["# model = MyModel(base_model, config, section_length, publication_length, X_2_length)\n","# model.load_state_dict(torch.load(config_2['save_path']))\n","# model.to(device)\n","\n","# # 分段訓練\n","# trainer2(model, train_loader, val_loader, config, device)\n","# trainer1(model, train_loader, val_loader, config, device)"]},{"cell_type":"markdown","metadata":{"id":"hW59mzIMvRNd"},"source":["## (5) Require_grad"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1703645945012,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"},"user_tz":-540},"id":"aWIEJzwCR3x0"},"outputs":[],"source":["# Freeze all layers\n","for param in model.base_model.parameters():\n","  param.requires_grad = False\n","\n","# Unfreeze part of layers\n","# for param in model.base_model.encoder.layer[6].parameters():\n","#     param.requires_grad = True\n","\n","# for param in model.base_model.encoder.layer[7].parameters():\n","#     param.requires_grad = True\n","\n","# for param in model.base_model.encoder.layer[8].parameters():\n","#     param.requires_grad = True\n","\n","# for param in model.base_model.encoder.layer[9].parameters():\n","#     param.requires_grad = True\n","\n","# for param in model.base_model.encoder.layer[10].parameters():\n","#     param.requires_grad = True\n","\n","for param in model.base_model.encoder.layer[11].parameters():\n","    param.requires_grad = True\n","\n","# for param in model.base_model.bert.encoder.layer[11].parameters():\n","#     param.requires_grad = True\n","\n","# for param in model.base_model.classifier.parameters():\n","#     param.requires_grad = True\n","\n","# for param in model.base_model.fc1.parameters():\n","#     param.requires_grad = True\n","\n","# for param in model.base_model.fc2.parameters():\n","#     param.requires_grad = True\n","\n","# for param in model.base_model.fc3.parameters():\n","    # param.requires_grad = True\n","\n","# Check requires_grad status\n","for name, param in model.named_parameters():\n","    print(name, param.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"ac2Ve-JWR7EX"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"Kr5YH0wHvMaG"},"source":["save based on acc and loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYmuBnnGYCXa","executionInfo":{"status":"aborted","timestamp":1703645945012,"user_tz":-540,"elapsed":16,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["def trainer(model, train_loader, val_loader, config, device):\n","\n","    criterion = config['criterion']\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n","\n","    writer = SummaryWriter()  # Writer of tensoboard.\n","    n_epochs, best_loss, best_acc, step, early_stop_count = config['n_epochs'], math.inf, 0, 0, 0\n","\n","    # 1. Training\n","    for epoch in range(n_epochs):\n","      model.train()  # Set the model to training mode\n","      loss_record = []\n","\n","      train_pbar = tqdm(train_loader, position=0, leave=True)  # tqdm is a package to visualize your training progress.\n","      for input_ids, attention_mask, X_2, y in train_loader:\n","        optimizer.zero_grad()  # Set gradient to zero\n","\n","        # Forward pass\n","        input_ids, attention_mask, X_2, y = input_ids.to(device), attention_mask.to(device), X_2.to(device), y.to(device)\n","        pred = model(input_ids, attention_mask, X_2)\n","        loss = criterion(pred, y)\n","        loss.backward()                     # Compute gradient(backpropagation).\n","        optimizer.step()                    # Update parameters.\n","        step += 1\n","        loss_record.append(loss.detach().item())\n","\n","        # Display current epoch number and loss on tqdm progress bar.\n","        train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n","        train_pbar.set_postfix({'loss': loss.detach().item()})\n","\n","      mean_train_loss = sum(loss_record)/len(loss_record)\n","      writer.add_scalar('Loss/train', mean_train_loss, step)\n","\n","      # 2. Evaluation\n","      model.eval() # Set your model to evaluation mode.\n","      loss_record = []\n","      predicted_labels_list = []\n","      targets_list = []\n","      for input_ids, attention_mask, X_2, y in val_loader:\n","          input_ids, attention_mask, X_2, y = input_ids.to(device), attention_mask.to(device), X_2.to(device), y.to(device)\n","          with torch.no_grad():\n","              pred = model(input_ids, attention_mask, X_2)\n","              _, predicted = torch.max(pred, 1)\n","              loss = criterion(pred, y)\n","              predicted_labels_list.extend(predicted.tolist())\n","              targets_list.extend(y.tolist())\n","              loss_record.append(loss.item())\n","      accuracy = accuracy_score(targets_list, predicted_labels_list)\n","\n","      # Mean\n","      mean_valid_loss = sum(loss_record)/len(loss_record)\n","      print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}, Val Acc: {accuracy:.4f}')\n","      writer.add_scalar('Loss/valid', mean_valid_loss, step)\n","\n","      # 3. Judge of saving model\n","      if (accuracy > best_acc) or ((accuracy == best_acc) and (mean_valid_loss < best_loss)):\n","          best_acc = accuracy\n","          best_loss = mean_valid_loss\n","          torch.save(model.state_dict(), config_2['save_path']) # Save your best model\n","          print('Saving model with loss {:.3f}...'.format(best_acc))\n","          early_stop_count = 0\n","      else:\n","          early_stop_count += 1\n","\n","      if early_stop_count >= config['early_stop']:\n","          print('\\nModel is not improving, so we halt the training session.')\n","          return\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eE3MJqOQvI--"},"source":["Old trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYCy6aMevHK1","executionInfo":{"status":"aborted","timestamp":1703645945013,"user_tz":-540,"elapsed":17,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["# def trainer(model, train_loader, val_loader, config, device):\n","\n","#     criterion = config['criterion']\n","\n","#     # ----------------------Learning Rate-----------------------\n","#     # optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n","#     optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n","\n","#     # learning_rates = {\n","#     #     'base_model': 1e-5,  # 设置base_model的学习率\n","#     #     'base_model_fc': 1e-5,\n","#     #     'other_params': 1e-3  # 设置其他参数的学习率\n","#     # }\n","\n","#     # param_groups = [\n","#     #     {'params': model.base_model.parameters(), 'lr': learning_rates['base_model']},\n","#     #     {'params': model.fc1.weight, 'lr': learning_rates['base_model_fc']},\n","#     #     {'params': model.fc1.bias, 'lr': learning_rates['base_model_fc']},\n","#     #     # {'params': model.fc2.weight, 'lr': learning_rates['base_model_fc']},\n","#     #     # {'params': model.fc2.bias, 'lr': learning_rates['base_model_fc']},\n","#     #     {'params': model.fc_h_c.weight, 'lr': learning_rates['base_model_fc']},\n","#     #     {'params': model.fc_h_c.bias, 'lr': learning_rates['base_model_fc']},\n","#     #     # {'params': model.fc_h_news.weight, 'lr': learning_rates['base_model_fc']},\n","#     #     # {'params': model.fc_h_news.bias, 'lr': learning_rates['base_model_fc']},\n","#     #     {'params': model.lstm_1.weight_ih_l0, 'lr': learning_rates['other_params']},\n","#     #     {'params': model.lstm_1.weight_hh_l0, 'lr': learning_rates['other_params']},\n","#     #     {'params': model.lstm_1.bias_ih_l0, 'lr': learning_rates['other_params']},\n","#     #     {'params': model.lstm_1.bias_hh_l0, 'lr': learning_rates['other_params']},\n","#     #     {'params': model.lstm_1.weight_ih_l1, 'lr': learning_rates['other_params']},\n","#     #     {'params': model.lstm_1.weight_hh_l1, 'lr': learning_rates['other_params']},\n","#     #     {'params': model.lstm_1.bias_ih_l1, 'lr': learning_rates['other_params']},\n","#     #     {'params': model.lstm_1.bias_hh_l1, 'lr': learning_rates['other_params']},\n","#     #     {'params': model.sequential[0].weight, 'lr': learning_rates['other_params']},\n","#     #     {'params': model.sequential[0].bias, 'lr': learning_rates['other_params']},\n","#     # ]\n","#     # optimizer = torch.optim.AdamW(param_groups)\n","#     # ----------------------------------------------\n","\n","#     writer = SummaryWriter()  # Writer of tensoboard.\n","#     n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n","\n","#     # 1. Training\n","#     for epoch in range(n_epochs):\n","#       model.train()  # Set the model to training mode\n","#       loss_record = []\n","\n","#       train_pbar = tqdm(train_loader, position=0, leave=True)  # tqdm is a package to visualize your training progress.\n","#       for input_ids, attention_mask, section, publication, X_2, y in train_loader:\n","#         optimizer.zero_grad()  # Set gradient to zero\n","\n","#         # Forward pass\n","#         input_ids, attention_mask, section, publication, X_2, y = input_ids.to(device), attention_mask.to(device), section.to(device), publication.to(device), X_2.to(device), y.to(device)\n","#         pred = model(input_ids, attention_mask, section, publication, X_2)\n","#         loss = criterion(pred, y)\n","#         loss.backward()                     # Compute gradient(backpropagation).\n","#         optimizer.step()                    # Update parameters.\n","#         step += 1\n","#         loss_record.append(loss.detach().item())\n","\n","#         # Display current epoch number and loss on tqdm progress bar.\n","#         train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n","#         train_pbar.set_postfix({'loss': loss.detach().item()})\n","\n","#       mean_train_loss = sum(loss_record)/len(loss_record)\n","#       writer.add_scalar('Loss/train', mean_train_loss, step)\n","\n","#       # 2. Evaluation\n","#       model.eval() # Set your model to evaluation mode.\n","#       loss_record = []\n","#       predicted_labels_list = []\n","#       targets_list = []\n","#       for input_ids, attention_mask, section, publication, X_2, y in val_loader:\n","#           input_ids, attention_mask, section, publication, X_2, y = input_ids.to(device), attention_mask.to(device), section.to(device), publication.to(device), X_2.to(device), y.to(device)\n","#           with torch.no_grad():\n","#               pred = model(input_ids, attention_mask, section, publication, X_2)\n","#               _, predicted = torch.max(pred, 1)\n","#               loss = criterion(pred, y)\n","#               predicted_labels_list.extend(predicted.tolist())\n","#               targets_list.extend(y.tolist())\n","#               loss_record.append(loss.item())\n","#       accuracy = accuracy_score(targets_list, predicted_labels_list)\n","\n","#       # Mean\n","#       mean_valid_loss = sum(loss_record)/len(loss_record)\n","#       print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}, Val Acc: {accuracy:.4f}')\n","#       writer.add_scalar('Loss/valid', mean_valid_loss, step)\n","\n","#       # 3. Judge of saving model\n","#       if mean_valid_loss < best_loss:\n","#           best_loss = mean_valid_loss\n","#           torch.save(model.state_dict(), config_2['save_path']) # Save your best model\n","#           print('Saving model with loss {:.3f}...'.format(best_loss))\n","#           early_stop_count = 0\n","#       else:\n","#           early_stop_count += 1\n","\n","#       if early_stop_count >= config['early_stop']:\n","#           print('\\nModel is not improving, so we halt the training session.')\n","#           return\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hU3BkXDPR5tK","executionInfo":{"status":"aborted","timestamp":1703645945013,"user_tz":-540,"elapsed":17,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["# 全部訓練\n","trainer(model, train_loader, val_loader, config, device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6s2pWKmMx6L","executionInfo":{"status":"aborted","timestamp":1703645945013,"user_tz":-540,"elapsed":17,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir=./runs/"]},{"cell_type":"markdown","metadata":{"id":"dRLIhxYobJPn"},"source":["# Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqPpm-mmC6MO","executionInfo":{"status":"aborted","timestamp":1703645945013,"user_tz":-540,"elapsed":17,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":["# Evaluation Dataloader\n","con_train_loader = DataLoader(train_dataset, batch_size=1, shuffle=config['shuffle'], drop_last=True, pin_memory=True)\n","con_val_loader = DataLoader(val_dataset, batch_size=1, shuffle=config['shuffle'], drop_last=True, pin_memory=True)\n","con_test_loader = DataLoader(test_dataset, batch_size=1, shuffle=config['shuffle'], drop_last=True, pin_memory=True)\n","\n","model = MyModel(base_model, config, element_size, X_2_length, batch_size=1)\n","model.load_state_dict(torch.load(config_2['save_path']))\n","model.to(device)\n","\n","# Evaluation mode\n","model.eval()\n","\n","accuracy_list = []\n","\n","# 1. Train part\n","with torch.no_grad():\n","    predicted_labels_list = []\n","    targets_list = []\n","    for input_ids, attention_mask, X_2, y in con_train_loader:\n","        input_ids, attention_mask, X_2, y = input_ids.to(device), attention_mask.to(device), X_2.to(device), y.to(device)\n","        outputs = model(input_ids, attention_mask, X_2)\n","        _, predicted_labels = torch.max(outputs, dim=1)  # 获取每个样本预测的类别索引\n","\n","        predicted_labels_list.extend(predicted_labels.tolist())\n","        targets_list.extend(y.tolist())\n","\n","    # 计算准确率\n","    accuracy = accuracy_score(targets_list, predicted_labels_list)\n","    accuracy_list.append(accuracy)\n","\n","print('=====================================================================================================================')\n","print('Training Result:')\n","print(classification_report(targets_list, predicted_labels_list))\n","print(confusion_matrix(targets_list, predicted_labels_list), '\\n')\n","print('macro_f1: ', f1_score(targets_list, predicted_labels_list, average='macro'))\n","print('weighted_f1: ', f1_score(targets_list, predicted_labels_list, average='weighted'))\n","\n","\n","# 2. Val part\n","with torch.no_grad():\n","    predicted_labels_list = []\n","    targets_list = []\n","    for input_ids, attention_mask, X_2, y in con_val_loader:\n","        input_ids, attention_mask, X_2, y = input_ids.to(device), attention_mask.to(device), X_2.to(device), y.to(device)\n","        outputs = model(input_ids, attention_mask, X_2)\n","        _, predicted_labels = torch.max(outputs, dim=1)  # 获取每个样本预测的类别索引\n","\n","        predicted_labels_list.extend(predicted_labels.tolist())\n","        targets_list.extend(y.tolist())\n","\n","    # 计算准确率\n","    accuracy = accuracy_score(targets_list, predicted_labels_list)\n","    accuracy_list.append(accuracy)\n","\n","print('=====================================================')\n","print('Val Result:')\n","print(classification_report(targets_list, predicted_labels_list))\n","print(confusion_matrix(targets_list, predicted_labels_list))\n","print('macro_f1: ', f1_score(targets_list, predicted_labels_list, average='macro'))\n","print('weighted_f1: ', f1_score(targets_list, predicted_labels_list, average='weighted'))\n","\n","# 3. Test part\n","with torch.no_grad():\n","    predicted_labels_list = []\n","    targets_list = []\n","    for input_ids, attention_mask, X_2, y in con_test_loader:\n","        input_ids, attention_mask, X_2, y = input_ids.to(device), attention_mask.to(device), X_2.to(device), y.to(device)\n","        outputs = model(input_ids, attention_mask, X_2)\n","        _, predicted_labels = torch.max(outputs, dim=1)  # 获取每个样本预测的类别索引\n","\n","        predicted_labels_list.extend(predicted_labels.tolist())\n","        targets_list.extend(y.tolist())\n","\n","    # 计算准确率\n","    accuracy = accuracy_score(targets_list, predicted_labels_list)\n","    accuracy_list.append(accuracy)\n","\n","print('=====================================================')\n","print('Testing Result:')\n","print(classification_report(targets_list, predicted_labels_list))\n","print(confusion_matrix(targets_list, predicted_labels_list))\n","print('macro_f1: ', f1_score(targets_list, predicted_labels_list, average='macro'))\n","print('weighted_f1: ', f1_score(targets_list, predicted_labels_list, average='weighted'))\n","\n","print('=====================================================', '\\n')\n","print(\"Accuracy [Train, Val, Test]: \", accuracy_list, '\\n')\n","print('Config: ', config, '\\n')\n","print('Feature: ', feature)\n","print('time_start: ', time_start, 'time_end: ', time_end)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFgOTaztjQ9z","executionInfo":{"status":"aborted","timestamp":1703645945013,"user_tz":-540,"elapsed":17,"user":{"displayName":"Yoga Liu","userId":"16295099566698695483"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}